{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariahelenass/AI-MODELS/blob/main/Copy_of_functiongemma_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FunctionGemma Fine-tuning for Flutter\n",
        "\n",
        "Fine-tuning notebook for [flutter_gemma](https://github.com/DenisovAV/flutter_gemma) plugin.\n",
        "\n",
        "Trains FunctionGemma 270M for custom function calling:\n",
        "- `change_background_color` - Changes app background color\n",
        "- `change_app_title` - Changes app title\n",
        "- `show_alert` - Shows alert dialog\n",
        "\n",
        "**Pipeline:**\n",
        "1. **This notebook** - Fine-tune model\n",
        "2. [functiongemma_to_tflite.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_to_tflite.ipynb) - Convert to TFLite\n",
        "3. [functiongemma_tflite_to_task.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_tflite_to_task.ipynb) - Bundle as .task for Flutter\n",
        "\n",
        "**Requirements:**\n",
        "- A100 GPU runtime (Runtime ‚Üí Change runtime type ‚Üí A100)\n",
        "- HuggingFace account with accepted [Gemma license](https://huggingface.co/google/functiongemma-270m-it)\n",
        "- HuggingFace token with write access"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "**What we're installing:**\n",
        "- `torch` - PyTorch, the core ML framework\n",
        "- `transformers` - HuggingFace library for working with LLMs\n",
        "- `trl` - Transformer Reinforcement Learning, for SFT (Supervised Fine-Tuning)\n",
        "- `datasets` - for dataset handling\n",
        "- `accelerate` - for distributed training and GPU optimization\n",
        "- `sentencepiece` - tokenizer for Gemma models\n",
        "\n",
        "**Versions are pinned** for reproducibility."
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Install dependencies (use Colab's pre-installed torch)\n",
        "# =============================================================================\n",
        "# Don't reinstall torch - Colab has optimized version pre-installed\n",
        "\n",
        "!pip install -q transformers==4.57.3 datasets accelerate evaluate trl==0.26.2 protobuf sentencepiece\n",
        "!pip install -q huggingface_hub tensorboard\n",
        "\n",
        "print(\"\\nDependencies installed!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. HuggingFace Authentication\n",
        "\n",
        "**Setup Colab Secret:**\n",
        "1. Click the üîë key icon in the left panel\n",
        "2. Add new secret: `HF_TOKEN`\n",
        "3. Paste your HuggingFace token (from https://huggingface.co/settings/tokens)\n",
        "4. Toggle \"Notebook access\" ON\n",
        "\n",
        "**Don't forget:** Accept the Gemma license at https://huggingface.co/google/functiongemma-270m-it"
      ],
      "metadata": {
        "id": "auth_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Authenticate with HuggingFace using Colab Secrets\n",
        "# =============================================================================\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Read token from Colab Secrets (key icon in left panel)\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    raise ValueError(\"HF_TOKEN not found in Colab Secrets. Add it via the üîë key icon.\")\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Logged in to HuggingFace!\")"
      ],
      "metadata": {
        "id": "auth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Upload Training Data\n",
        "\n",
        "**File:** `training_data.jsonl` (320 examples)\n",
        "\n",
        "**Format per line:**\n",
        "```json\n",
        "{\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", \"tool_arguments\": \"{\\\"color\\\": \\\"red\\\"}\"}\n",
        "```\n",
        "\n",
        "**How to upload:**\n",
        "1. Drag and drop the file into the left panel (Files)\n",
        "2. Or run this cell - an Upload button will appear"
      ],
      "metadata": {
        "id": "upload_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "if not os.path.exists('training_data.jsonl'):\n",
        "    print(\"Please upload training_data.jsonl:\")\n",
        "    uploaded = files.upload()\n",
        "else:\n",
        "    print(\"‚úÖ training_data.jsonl already exists\")\n",
        "\n",
        "# Verify file\n",
        "!wc -l training_data.jsonl\n",
        "!head -1 training_data.jsonl"
      ],
      "metadata": {
        "id": "upload_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define Tools and Prepare Dataset\n",
        "\n",
        "### 4.1 Define Functions (Tools)\n",
        "\n",
        "We define Python functions with type hints and docstrings. The `get_json_schema()` utility from HuggingFace transformers automatically generates JSON Schema for each function.\n",
        "\n",
        "**Why this approach:**\n",
        "- Type hints ‚Üí parameter types in schema\n",
        "- Docstrings ‚Üí function descriptions\n",
        "- No manual JSON writing needed"
      ],
      "metadata": {
        "id": "prepare_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers.utils import get_json_schema\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4.1: Define Python functions for JSON Schema generation\n",
        "# =============================================================================\n",
        "# These functions are NOT executed - they're only used for JSON Schema generation.\n",
        "# get_json_schema() reads the function name, docstring, and type hints,\n",
        "# and creates a JSON Schema in OpenAI function calling format.\n",
        "\n",
        "def change_background_color(color: str) -> str:\n",
        "    \"\"\"Changes the app background color to specified color.\n",
        "\n",
        "    Args:\n",
        "        color: The color name (red, green, blue, yellow, purple, orange)\n",
        "    \"\"\"\n",
        "    return f\"Changed to {color}\"\n",
        "\n",
        "def change_app_title(title: str) -> str:\n",
        "    \"\"\"Changes the application title text in the AppBar.\n",
        "\n",
        "    Args:\n",
        "        title: The new title text to display\n",
        "    \"\"\"\n",
        "    return f\"Title set to {title}\"\n",
        "\n",
        "def show_alert(title: str, message: str) -> str:\n",
        "    \"\"\"Shows an alert dialog with a custom message and title.\n",
        "\n",
        "    Args:\n",
        "        title: The title of the alert dialog\n",
        "        message: The message content of the alert dialog\n",
        "    \"\"\"\n",
        "    return f\"Alert shown: {title}\"\n",
        "\n",
        "# =============================================================================\n",
        "# Generate JSON Schemas from Python functions\n",
        "# =============================================================================\n",
        "# get_json_schema() creates a structure like:\n",
        "# {\"type\": \"function\", \"function\": {\"name\": \"...\", \"description\": \"...\", \"parameters\": {...}}}\n",
        "\n",
        "TOOLS = [\n",
        "    get_json_schema(change_background_color),\n",
        "    get_json_schema(change_app_title),\n",
        "    get_json_schema(show_alert),\n",
        "]\n",
        "\n",
        "print(\"Tools defined:\")\n",
        "for tool in TOOLS:\n",
        "    print(f\"   - {tool['function']['name']}: {tool['function']['description'][:50]}...\")"
      ],
      "metadata": {
        "id": "define_tools"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Convert Dataset to Google FunctionGemma Format\n",
        "\n",
        "We use the **official Google FunctionGemma format** as documented at:\n",
        "https://huggingface.co/google/functiongemma-270m-it\n",
        "\n",
        "We do NOT use HuggingFace's `apply_chat_template` because it produces a different format\n",
        "that breaks compatibility with the base model.\n",
        "\n",
        "**Google FunctionGemma format:**\n",
        "\n",
        "**Input (prompt):**\n",
        "```\n",
        "<start_of_turn>developer\n",
        "You are a model that can do function calling with the following functions\n",
        "<start_function_declaration>declaration:function_name{description:<escape>...<escape>,parameters:{...}}<end_function_declaration>\n",
        "<end_of_turn>\n",
        "<start_of_turn>user\n",
        "make it red\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "```\n",
        "\n",
        "**Output (completion):**\n",
        "```\n",
        "<start_function_call>call:function_name{param:<escape>value<escape>}<end_function_call>\n",
        "```\n",
        "\n",
        "**Key format elements:**\n",
        "- `<escape>` tokens wrap all string values (not JSON quotes)\n",
        "- `call:` prefix before function name in output\n",
        "- Parameters use `{param:<escape>value<escape>}` format"
      ],
      "metadata": {
        "id": "FQra1piRzQlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STEP 4.2: Convert dataset to Google FunctionGemma format\n",
        "# =============================================================================\n",
        "# CRITICAL: We manually create prompts in the EXACT format Flutter uses!\n",
        "# Do NOT use apply_chat_template - it produces a different format.\n",
        "\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# FunctionGemma special tokens (same as Flutter uses)\n",
        "START_TURN = \"<start_of_turn>\"\n",
        "END_TURN = \"<end_of_turn>\"\n",
        "START_DECL = \"<start_function_declaration>\"\n",
        "END_DECL = \"<end_function_declaration>\"\n",
        "START_CALL = \"<start_function_call>\"\n",
        "END_CALL = \"<end_function_call>\"\n",
        "ESCAPE = \"<escape>\"\n",
        "\n",
        "# =============================================================================\n",
        "# Function declarations in Google format (same as Flutter's _createFunctionGemmaToolsPrompt)\n",
        "# =============================================================================\n",
        "FUNCTION_DECLARATIONS = f\"\"\"{START_DECL}declaration:change_background_color{{description:{ESCAPE}Changes the app background color{ESCAPE},parameters:{{properties:{{color:{{description:{ESCAPE}The color name (red, green, blue, yellow, purple, orange){ESCAPE},type:{ESCAPE}STRING{ESCAPE}}}}},required:[{ESCAPE}color{ESCAPE}],type:{ESCAPE}OBJECT{ESCAPE}}}}}{END_DECL}\n",
        "{START_DECL}declaration:change_app_title{{description:{ESCAPE}Changes the application title text in the AppBar{ESCAPE},parameters:{{properties:{{title:{{description:{ESCAPE}The new title text to display{ESCAPE},type:{ESCAPE}STRING{ESCAPE}}}}},required:[{ESCAPE}title{ESCAPE}],type:{ESCAPE}OBJECT{ESCAPE}}}}}{END_DECL}\n",
        "{START_DECL}declaration:show_alert{{description:{ESCAPE}Shows an alert dialog with a custom message and title{ESCAPE},parameters:{{properties:{{title:{{description:{ESCAPE}The title of the alert dialog{ESCAPE},type:{ESCAPE}STRING{ESCAPE}}},message:{{description:{ESCAPE}The message content of the alert dialog{ESCAPE},type:{ESCAPE}STRING{ESCAPE}}}}},required:[{ESCAPE}title{ESCAPE},{ESCAPE}message{ESCAPE}],type:{ESCAPE}OBJECT{ESCAPE}}}}}{END_DECL}\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"{START_TURN}developer\n",
        "You are a model that can do function calling with the following functions\n",
        "{FUNCTION_DECLARATIONS}\n",
        "{END_TURN}\n",
        "\"\"\"\n",
        "\n",
        "def create_training_example(sample):\n",
        "    \"\"\"\n",
        "    Creates training example in exact Google FunctionGemma format.\n",
        "\n",
        "    Input: {\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", \"tool_arguments\": \"{\\\"color\\\": \\\"red\\\"}\"}\n",
        "\n",
        "    Output text for training:\n",
        "    <start_of_turn>developer\n",
        "    You are a model...\n",
        "    <end_of_turn>\n",
        "    <start_of_turn>user\n",
        "    make it red\n",
        "    <end_of_turn>\n",
        "    <start_of_turn>model\n",
        "    <start_function_call>call:change_background_color{color:<escape>red<escape>}<end_function_call>\n",
        "    \"\"\"\n",
        "    user_content = sample[\"user_content\"]\n",
        "    tool_name = sample[\"tool_name\"]\n",
        "    tool_args = json.loads(sample[\"tool_arguments\"])\n",
        "\n",
        "    # Build prompt (input)\n",
        "    prompt = f\"\"\"{SYSTEM_PROMPT}{START_TURN}user\n",
        "{user_content}\n",
        "{END_TURN}\n",
        "{START_TURN}model\n",
        "\"\"\"\n",
        "\n",
        "    # Build completion (output) in Google format: {param:<escape>value<escape>}\n",
        "    params_str = \",\".join([f\"{k}:{ESCAPE}{v}{ESCAPE}\" for k, v in tool_args.items()])\n",
        "    completion = f\"{START_CALL}call:{tool_name}{{{params_str}}}{END_CALL}\"\n",
        "\n",
        "    # Full training text = prompt + completion\n",
        "    return {\"text\": prompt + completion}\n",
        "\n",
        "# =============================================================================\n",
        "# Load and convert dataset\n",
        "# =============================================================================\n",
        "raw_data = []\n",
        "with open('training_data.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        raw_data.append(json.loads(line.strip()))\n",
        "\n",
        "print(f\"Loaded {len(raw_data)} raw examples\")\n",
        "\n",
        "dataset = Dataset.from_list(raw_data)\n",
        "dataset = dataset.map(create_training_example, remove_columns=dataset.features)\n",
        "\n",
        "# Split into train/test (90%/10%)\n",
        "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
        "\n",
        "print(f\"\\nDataset prepared:\")\n",
        "print(f\"   Train: {len(dataset['train'])} examples\")\n",
        "print(f\"   Test:  {len(dataset['test'])} examples\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Sample training example:\")\n",
        "print(\"=\"*60)\n",
        "print(dataset['train'][0]['text'][:800])\n",
        "print(\"...\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load Base Model\n",
        "\n",
        "**Model:** `google/functiongemma-270m-it`\n",
        "- 270M parameters (compact, designed for on-device)\n",
        "- Instruction-tuned (it) - already trained to follow instructions\n",
        "- Specialized for function calling\n",
        "\n",
        "**Loading parameters:**\n",
        "- `torch_dtype=bfloat16` - 16-bit weights to save memory (~540MB instead of ~1GB)\n",
        "- `device_map=\"auto\"` - automatically load to GPU\n",
        "- `attn_implementation=\"eager\"` - without FlashAttention (for compatibility)"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# =============================================================================\n",
        "# Load FunctionGemma base model\n",
        "# =============================================================================\n",
        "BASE_MODEL = \"google/functiongemma-270m-it\"\n",
        "\n",
        "print(f\"Loading {BASE_MODEL}...\")\n",
        "print(\"   (Downloads ~540MB on first run, then uses cache)\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.bfloat16,      # 16-bit to save VRAM\n",
        "    device_map=\"auto\",                # Automatically load to GPU\n",
        "    attn_implementation=\"eager\"       # Without FlashAttention for compatibility\n",
        ")\n",
        "\n",
        "# Tokenizer converts text to tokens and back\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "print(f\"\\nModel loaded!\")\n",
        "print(f\"   Parameters: {model.num_parameters():,}\")\n",
        "print(f\"   Memory: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")\n",
        "print(f\"   Device: {model.device}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Configure Training\n",
        "\n",
        "**Hyperparameters from official Google FunctionGemma cookbook:**\n",
        "\n",
        "| Parameter | Value | Explanation |\n",
        "|-----------|-------|-------------|\n",
        "| `num_train_epochs` | 5 | Extended training for enum support (320 examples) |\n",
        "| `learning_rate` | 1e-5 | Learning rate (conservative for fine-tuning) |\n",
        "| `lr_scheduler_type` | cosine | Smoothly decreases LR towards end of training |\n",
        "| `gradient_accumulation_steps` | 8 | Gradient accumulation (effective batch = 32) |\n",
        "| `max_length` | 1024 | Maximum sequence length in tokens |\n",
        "| `bf16` | True | 16-bit training to save memory |\n",
        "\n",
        "**Why these values:**\n",
        "- LR 1e-5 (not 5e-5) - prevents \"forgetting\" base knowledge\n",
        "- Cosine scheduler - smooth LR decay improves convergence\n",
        "- Gradient accumulation 8 - simulates large batch without running out of memory"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"functiongemma-flutter-demo\"\n",
        "\n",
        "# =============================================================================\n",
        "# Training configuration (based on official Google FunctionGemma cookbook)\n",
        "# https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/\n",
        "# =============================================================================\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # Dataset field with pre-formatted Google FunctionGemma format\n",
        "    dataset_text_field=\"text\",          # Use our pre-formatted text, NOT apply_chat_template\n",
        "\n",
        "    # Training params (Google official uses 2 epochs, we use 5 for enum support)\n",
        "    max_length=1024,                    # Max sequence length in tokens\n",
        "    packing=False,                      # Don't pack multiple examples into one sequence\n",
        "    num_train_epochs=5,                 # Extended training for enum support (320 examples)\n",
        "    per_device_train_batch_size=4,      # Batch size per GPU\n",
        "    per_device_eval_batch_size=4,       # Eval batch size\n",
        "    gradient_accumulation_steps=8,      # Effective batch size: 4 * 8 = 32\n",
        "\n",
        "    # Optimizer (Google official params)\n",
        "    learning_rate=1e-5,                 # Google official: 1e-5 (more conservative than 5e-5)\n",
        "    lr_scheduler_type=\"cosine\",         # Google official: cosine decay\n",
        "    optim=\"adamw_torch_fused\",          # Fused AdamW for faster training\n",
        "    warmup_ratio=0.1,                   # 10% warmup steps\n",
        "\n",
        "    # Logging and checkpoints\n",
        "    logging_steps=10,                   # Log every 10 steps\n",
        "    eval_strategy=\"epoch\",              # Evaluate after each epoch\n",
        "    save_strategy=\"epoch\",              # Save checkpoint after each epoch\n",
        "\n",
        "    # Memory optimization\n",
        "    gradient_checkpointing=False,       # Trade compute for memory (enable if OOM)\n",
        "    bf16=True,                          # Use bfloat16 for training\n",
        "\n",
        "    # Output\n",
        "    report_to=\"tensorboard\",            # Log to TensorBoard\n",
        "    push_to_hub=False,                  # Set to True to upload to HuggingFace\n",
        ")\n",
        "\n",
        "print(\"Training configuration (Google official params):\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   LR scheduler: {training_args.lr_scheduler_type}\")\n",
        "print(f\"   Max length: {training_args.max_length}\")\n",
        "print(f\"   Dataset field: {training_args.dataset_text_field}\")"
      ],
      "metadata": {
        "id": "config_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Start Training\n",
        "\n",
        "**What happens:**\n",
        "1. `SFTTrainer` uses the pre-formatted `text` field from our dataset\n",
        "2. Model learns to predict the correct function call for each user message\n",
        "3. We do NOT use `apply_chat_template` - data is already in correct Google format!\n",
        "\n",
        "**Training time:** ~5 minutes on A100 GPU (for ~300 examples)\n",
        "\n",
        "**Monitoring:**\n",
        "- `loss` should decrease\n",
        "- `eval_loss` should not increase (otherwise overfitting)"
      ],
      "metadata": {
        "id": "train_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Create SFTTrainer and start training\n",
        "# =============================================================================\n",
        "# dataset_text_field=\"text\" is configured in SFTConfig above.\n",
        "# This tells SFTTrainer to use our pre-formatted text directly,\n",
        "# without applying HuggingFace's chat template (which uses wrong format).\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    processing_class=tokenizer,  # TRL 0.26.2: use processing_class, not tokenizer\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"   Train examples: {len(dataset['train'])}\")\n",
        "print(f\"   Eval examples: {len(dataset['test'])}\")\n",
        "print(f\"   Format: Google FunctionGemma (manual)\")\n",
        "print(f\"   Estimated time: ~5 minutes on A100\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Train!\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Training complete!\")\n",
        "print(f\"   Final loss: {train_result.training_loss:.4f}\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Save Model\n",
        "\n",
        "**Files saved:**\n",
        "- `model.safetensors` - model weights (~540MB)\n",
        "- `config.json` - architecture configuration\n",
        "- `tokenizer.json`, `tokenizer_config.json` - tokenizer\n",
        "- `special_tokens_map.json` - special tokens\n",
        "\n",
        "**Format:** SafeTensors (safe, no pickle)"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Save the fine-tuned model to Google Drive\n",
        "# =============================================================================\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FINAL_MODEL_DIR = f\"{OUTPUT_DIR}-final\"\n",
        "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{FINAL_MODEL_DIR}\"\n",
        "\n",
        "# Save model weights and config\n",
        "trainer.save_model(FINAL_MODEL_DIR)\n",
        "\n",
        "# Save tokenizer (needed for inference)\n",
        "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "print(f\"Model saved locally to {FINAL_MODEL_DIR}/\")\n",
        "\n",
        "# Copy to Google Drive\n",
        "!cp -r {FINAL_MODEL_DIR} /content/drive/MyDrive/\n",
        "\n",
        "print(f\"\\nModel copied to Google Drive: {DRIVE_MODEL_DIR}/\")\n",
        "print(\"You can now use this in the conversion notebook!\")\n",
        "!ls -la {DRIVE_MODEL_DIR}/"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Test Fine-tuned Model\n",
        "\n",
        "**Important:** We test on NEW prompts that were NOT in the training dataset!\n",
        "\n",
        "We check:\n",
        "- Does the model choose the correct function?\n",
        "- Are the arguments correct?\n",
        "- Are there any hallucinations?"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Test the fine-tuned model on new prompts\n",
        "# =============================================================================\n",
        "# CRITICAL: Use the same Google format as training (not apply_chat_template!)\n",
        "\n",
        "test_prompts = [\n",
        "    \"make the background red\",       # change_background_color\n",
        "    \"rename the app to Hello World\", # change_app_title\n",
        "    \"show an alert saying welcome\",  # show_alert\n",
        "    \"I want a purple background\",    # Variation for color\n",
        "    \"set title to My App\",           # Variation for title\n",
        "]\n",
        "\n",
        "print(\"Testing fine-tuned model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    # Create prompt in SAME format as training (Google FunctionGemma)\n",
        "    input_text = f\"\"\"{SYSTEM_PROMPT}{START_TURN}user\n",
        "{prompt}\n",
        "{END_TURN}\n",
        "{START_TURN}model\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize and send to GPU\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode only new tokens (without prompt)\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nUser: {prompt}\")\n",
        "    print(f\"Model: {response.strip()}\")\n",
        "\n",
        "    # Verify format\n",
        "    if \"<start_function_call>call:\" in response:\n",
        "        print(\"   ‚úÖ Correct format!\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  Unexpected format\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Download Model\n",
        "\n",
        "**Download ZIP archive** with all model files.\n",
        "\n",
        "**Next steps after downloading:**\n",
        "1. **Convert to TFLite** - using `ai-edge-torch`\n",
        "2. **Bundle as .task** - using MediaPipe Model Bundler\n",
        "3. **Integrate into Flutter** - put in assets or download from server"
      ],
      "metadata": {
        "id": "MPKv8OF7zQlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Zip and download the model\n",
        "# =============================================================================\n",
        "!zip -r {FINAL_MODEL_DIR}.zip {FINAL_MODEL_DIR}/\n",
        "\n",
        "from google.colab import files\n",
        "files.download(f\"{FINAL_MODEL_DIR}.zip\")\n",
        "\n",
        "print(f\"\\nDownload started: {FINAL_MODEL_DIR}.zip\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Convert to TFLite using ai-edge-torch\")\n",
        "print(\"2. Bundle as .task using MediaPipe bundler\")\n",
        "print(\"3. Add to flutter_gemma example app\")"
      ],
      "metadata": {
        "id": "L5TLChHSzQlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Push to HuggingFace Hub"
      ],
      "metadata": {
        "id": "hub_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to push to HuggingFace Hub\n",
        "# Replace 'your-username' with your HuggingFace username\n",
        "\n",
        "# HUB_MODEL_ID = \"your-username/functiongemma-flutter-demo\"\n",
        "# trainer.push_to_hub(HUB_MODEL_ID)\n",
        "# print(f\"‚úÖ Model pushed to https://huggingface.co/{HUB_MODEL_ID}\")"
      ],
      "metadata": {
        "id": "push_hub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}